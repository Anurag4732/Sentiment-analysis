{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "yRp7HDycrlqL",
    "outputId": "5b0720c9-1d98-469d-d70d-dffefd1f372b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, re, scipy\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "og2wFeVHr8Vj",
    "outputId": "161354d7-dfb9-45e9-ed62-e61061f8397d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #pregnancy test https://goo.gl/h1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>finally a transparant silicon case ^^ thanks t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>we love this! would you go? #talk #makememorie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0  #fingerprint #pregnancy test https://goo.gl/h1...\n",
       "1   2      0  finally a transparant silicon case ^^ thanks t...\n",
       "2   3      0  we love this! would you go? #talk #makememorie..."
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sentiment_data_analytic_vidya.csv')\n",
    "df['tweet'] = df['tweet'].map(lambda x : x.lower())\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZFPi2LFD9Ws7",
    "outputId": "d2c43d92-93c8-4fc8-a42e-8648a2e7703e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfmg_SbE9W_n"
   },
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODkR_s7j9XN7"
   },
   "outputs": [],
   "source": [
    "def punch(x):\n",
    "    x = re.sub(r\"[-()\\\"#/@;:@*<>{}`ð'+=~£|¦â.!%+-^$?,0-9]\",\"\", x)\n",
    "    x = re.sub('$&@*#', 'vulgar', x) # Mention in data \n",
    "    x = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", x) #expand 'k' to '000' eg. 50k to 50000\n",
    "    x = re.sub(\"\\\\n\", ' ', x)\n",
    "    x = re.sub(\"\\W\", ' ', x)\n",
    "    x = re.sub(\"\\'s\", \" \", x) \n",
    "    x = re.sub(\"whats\", \"what is\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(\"\\'ve\", \"have\", x)\n",
    "    x = re.sub(\"can't\", \"can not\", x)\n",
    "    x = re.sub(\"n't\", \"not\", x)\n",
    "    x = re.sub(\"i'm\", \"i am\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"\\'re\", \"are\", x)\n",
    "    x = re.sub(\"\\'d\", \"would\", x)\n",
    "    x = re.sub(r\"http\\S+\", \"\", str(x))\n",
    "    x = re.sub(\"e\\.g\\.\", \"eg\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"e-mail\", \"email\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(r\"e - mail\", \"email\", x)\n",
    "    x = re.sub(r\"US\", \"America\", x)\n",
    "    x = re.sub(r\"USA\", \"America\", x)\n",
    "    x = re.sub(r\"us\", \"America\", x)\n",
    "    x = re.sub(r\"usa\", \"America\", x)\n",
    "    x = re.sub(r\"Chinese\", \"China\", x)\n",
    "    x = re.sub(r\"india\", \"India\", x)\n",
    "    x = re.sub(\"im\", \"i am\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"its\", \"it is\", x, flags= re.IGNORECASE)\n",
    "    x = re.sub(\"shes\", \"she is\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"u\", \"you\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"ur\", \"you are\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"wouldnt\", \"would not\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"youve\",  \"you have\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(r\"\\s{2,}\", \" \", x) # Remove extra space between words\n",
    "    x = x.strip() # Remove extra space from begning and ending\n",
    "    \n",
    "    return(x)\n",
    "\n",
    "df1['tweet'] = df1['tweet'].map(lambda x : punch(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2A00ufO69Uyx"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(x):\n",
    "    return(' '.join([word for word in nltk.word_tokenize(x) if word not in stopwords.words('english')]))\n",
    "\n",
    "def get_char_length_ratio(x): #To find how much word has meaningful content\n",
    "    return len(x['tweet'])/max(1,len(x['tweet_without_stop_words']))\n",
    "\n",
    "def get_Levenshtein(string1,string2): # Calculate Levenshtein distance to measure similiarity between string(edit base)\n",
    "    import editdistance\n",
    "    return editdistance.eval(string1,string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iZXlD11s-Wts"
   },
   "outputs": [],
   "source": [
    "df1['tweet_without_stop_words'] = df1['tweet'].map(lambda x : remove_stopwords(x))\n",
    "df1['char_length_ratio'] = df1.apply(lambda x : get_char_length_ratio(x), axis = 1)\n",
    "df1['leve_distance'] = df1.apply(lambda x : get_Levenshtein(x['tweet'], x['tweet_without_stop_words']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zh5pIuz3_X1u"
   },
   "outputs": [],
   "source": [
    "def lemitize(x):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  return(' '.join([lemmatizer.lemmatize(word)  for word in nltk.word_tokenize(x)]))\n",
    "df1['lemitized words'] = df1['tweet_without_stop_words'].map(lambda x : lemitize(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "fa3b7J0gPo2P",
    "outputId": "f5b75b8d-88b1-42af-9c68-21fbda65a800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Lexicon model based rating:-\n",
    "from nltk.corpus import opinion_lexicon\n",
    "nltk.download('opinion_lexicon')\n",
    "pos_list = set(opinion_lexicon.positive())\n",
    "neg_list = set(opinion_lexicon.negative())\n",
    "\n",
    "sentiment = []\n",
    "for x in df1['tweet_without_stop_words']:\n",
    "    marks = 0\n",
    "    for j in nltk.word_tokenize(x):\n",
    "        if j in pos_list:\n",
    "            marks += 1\n",
    "        elif j in neg_list:\n",
    "            marks -= 1\n",
    "    sentiment.append(marks)\n",
    "df1['lexicon_rating'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "R5eHr5dBPpEi",
    "outputId": "14f8ae54-a6c9-499d-de28-788ed414733b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7920x2276 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 66610 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = 'english', max_df = 0.99, strip_accents = 'unicode', ngram_range=(1, 1), min_df = 5) # n_gram range(1,2), (1,4) not improving accurecy\n",
    "temp = tfidf.fit_transform(df1['tweet']) #Remove stop words by self \n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "4Mtrs2RTPpMA",
    "outputId": "16e48947-cc97-432f-8eb9-70f2500db084"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>2240</th>\n",
       "      <th>2241</th>\n",
       "      <th>2242</th>\n",
       "      <th>2243</th>\n",
       "      <th>2244</th>\n",
       "      <th>2245</th>\n",
       "      <th>2246</th>\n",
       "      <th>2247</th>\n",
       "      <th>2248</th>\n",
       "      <th>2249</th>\n",
       "      <th>2250</th>\n",
       "      <th>2251</th>\n",
       "      <th>2252</th>\n",
       "      <th>2253</th>\n",
       "      <th>2254</th>\n",
       "      <th>2255</th>\n",
       "      <th>2256</th>\n",
       "      <th>2257</th>\n",
       "      <th>2258</th>\n",
       "      <th>2259</th>\n",
       "      <th>2260</th>\n",
       "      <th>2261</th>\n",
       "      <th>2262</th>\n",
       "      <th>2263</th>\n",
       "      <th>2264</th>\n",
       "      <th>2265</th>\n",
       "      <th>2266</th>\n",
       "      <th>2267</th>\n",
       "      <th>2268</th>\n",
       "      <th>2269</th>\n",
       "      <th>2270</th>\n",
       "      <th>2271</th>\n",
       "      <th>2272</th>\n",
       "      <th>2273</th>\n",
       "      <th>2274</th>\n",
       "      <th>2275</th>\n",
       "      <th>char_length_ratio</th>\n",
       "      <th>leve_distance</th>\n",
       "      <th>lexicon_rating</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.131579</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.195124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.091954</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3  ...  char_length_ratio  leve_distance  lexicon_rating  label\n",
       "0  0.0  0.0  0.0  0.0  ...           1.000000              0               0      0\n",
       "1  0.0  0.0  0.0  0.0  ...           1.131579             10               1      0\n",
       "2  0.0  0.0  0.0  0.0  ...           1.091954              8               1      0\n",
       "\n",
       "[3 rows x 2280 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_new = pd.DataFrame(temp.toarray())\n",
    "df1_new['char_length_ratio'] = df1['char_length_ratio']\n",
    "df1_new['leve_distance'] = df1['leve_distance']\n",
    "df1_new['lexicon_rating'] = df1['lexicon_rating']\n",
    "df1_new['label'] = df1['label']\n",
    "\n",
    "df1_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JyIZc09RPo_K",
    "outputId": "4835d883-2a91-4729-edd8-e18b61299c75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 2280)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ads4BcqJ-W9h"
   },
   "outputs": [],
   "source": [
    "x_dev, x_final, y_dev, y_final = train_test_split(df1_new.iloc[:,:-1], df1_new['label'], test_size = 0.08, random_state = 42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_dev, y_dev, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nGUaTjJ98Qau",
    "outputId": "f9433177-858f-434d-923c-18b221d9e9ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7286, 2279), (634, 2279))"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dev.shape,x_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AsvmshE_vHOJ",
    "outputId": "db567488-4742-4f78-f2b8-8de7077dc80f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5828, 2279), (5828,))"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "y8lLULr4QGvA",
    "outputId": "5ce881d1-3df2-408e-b146-6b583ebf20eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=50, max_features='sqrt',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=430,\n",
       "                       n_jobs=None, oob_score=True, random_state=42, verbose=0,\n",
       "                       warm_start=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                       max_depth=50, max_features='sqrt', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators = 430,\n",
    "                       n_jobs=None, oob_score = True, random_state=42, verbose=0,\n",
    "                       warm_start= True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "izRszqMmQGmb",
    "outputId": "e15904a7-a210-4180-8a0e-2a2380f5c745"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7772277227722771"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "y_pred = []\n",
    "for i in pred:\n",
    "    if i <= 0.5:\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)\n",
    "\n",
    "f1_score(y_test, np.array(y_pred).reshape(y_test.shape))\n",
    "# 0.76649 (1, 1) min_df = 3\n",
    "# 0.775 min_df = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eC7v07NFQGgR",
    "outputId": "f485d73d-1617-4e4e-9cda-3b30b3dd20ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[545,  34],\n",
       "       [ 56, 157]])"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6yFu9ztzw5lp",
    "outputId": "c45064e8-bf41-4928-aab1-41a172e19349"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7128, 2279), (792, 2279))"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZLcfe4wmvYU"
   },
   "outputs": [],
   "source": [
    "############         ANN             #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "gNPGxxi1w5h2",
    "outputId": "f179cbc4-8f18-40f1-c3a0-97f019b2bab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7660668380462724\n",
      "0.7571801566579635\n",
      "0.764857881136951\n",
      "Avg:- 0.45762097516823735\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "\n",
    "# To avoid randomness:- It is not totally removing the result difference problem of every iteration only \n",
    "# reducing some difference or can say some values are fixed, and some are also changing so by fixing some\n",
    "# by using these It settle down.\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "result = 0\n",
    "\n",
    "for i in range(0,3):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(800, input_dim = 2279, activation= 'relu'))  # 500, 500, 1, 64\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(500, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  from keras.wrappers.scikit_learn import KerasClassifier\n",
    "  import keras.backend as K\n",
    "\n",
    "  def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "      true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "      possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "      predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "      precision = true_positives / (predicted_positives + K.epsilon())\n",
    "      recall = true_positives / (possible_positives + K.epsilon())\n",
    "      f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "      return f1_val\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[get_f1])\n",
    "  # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  # l = [5, 10,  50]\n",
    "\n",
    "  history = model.fit(x_train, y_train, epochs = 5 , batch_size = 32, verbose = 0)\n",
    "\n",
    "  pred = model.predict(x_test)\n",
    "\n",
    "  y_pred = []\n",
    "  for i in pred:\n",
    "    if i <= 0.5:\n",
    "      y_pred.append(0)\n",
    "    else:\n",
    "      y_pred.append(1)\n",
    "\n",
    "  print(f1_score(y_test, np.array(y_pred).reshape(y_test.shape)))\n",
    "  result += f1_score(y_test, np.array(y_pred).reshape(y_test.shape))\n",
    "\n",
    "  confusion_matrix(y_test, y_pred)\n",
    "print('Avg:-', result/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6mvqIfNp8d48",
    "outputId": "9f77c3bd-6d62-4682-b1a5-43397b347453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7963525835866262\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_final)\n",
    "\n",
    "y_pred = []\n",
    "for i in pred:\n",
    "  if i <= 0.5:\n",
    "    y_pred.append(0)\n",
    "  else:\n",
    "    y_pred.append(1)\n",
    "\n",
    "print(f1_score(y_final, np.array(y_pred).reshape(y_final.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJERP1L78dkc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cr5GSSxohAMM"
   },
   "outputs": [],
   "source": [
    "## Result:-\n",
    "1.epochs = 5, F1-Score = 0.8245\n",
    "  model.add(Dense(800, input_dim = 2279, activation='relu'))\n",
    "\n",
    "  model.add(Dense(300, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "2. epochs = 5, batch = 16, f1 = 0.8018, layer = [500, 500, 1], activation = ['relu', 'relu', 'sigmoid']\n",
    "3. epochs = 5, batch = 16, f1 = 0.813, layer = [500, 500, 1], activation = ['sigmoid', 'sigmoid', 'sigmoid']\n",
    "3. epochs = 5, batch = 64, f1 = 0.800, layer = [800, 800, 1], activation = ['sigmoid', 'sigmoid', 'sigmoid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9q6H0DVJhAGm"
   },
   "outputs": [],
   "source": [
    "########################       LSTM         #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dC6ChYZyE5Wq"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import SpatialDropout1D, Dropout\n",
    "from keras.layers import Dense, Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "MaHsqPDKKj3v",
    "outputId": "b94c5a55-afc9-46e5-a592-fef4a1f362d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<7920x2276 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 66610 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for i in df1['tweet_without_stop_words']:\n",
    "  l.extend(nltk.word_tokenize(i))\n",
    "print(len(set(l)))  # Total words\n",
    "\n",
    "# tf-idf helps to find no of top important(Occuring) words(min_df, max_df) which use as max_feature:\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', max_df = 0.99, strip_accents = 'unicode', ngram_range=(1, 1), min_df = 5) # n_gram range(1,2), (1,4) not improving accurecy\n",
    "temp = tfidf.fit_transform(df1['tweet']) #Remove stop words by self \n",
    "temp\n",
    "\n",
    "# So we take approx 2280 words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "Kba00UBpE5Pd",
    "outputId": "7e99afb7-712e-4452-ad9d-b87080061435"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2072, 1213, 15, 63, 40, 21, 102, 78, 91, 84, 1],\n",
       " [56, 33, 74, 195, 7, 238],\n",
       " [11, 79, 5, 99, 837, 429, 1, 103, 270, 1112],\n",
       " [2073, 122, 180, 271, 1, 21, 80],\n",
       " [60, 311, 2, 152, 142, 837, 72, 430, 196, 356]]"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and Create Sequence\n",
    "max_fatures = 2280            # top 2000 words\n",
    "tokenizer = Tokenizer(num_words = max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(df1['tweet_without_stop_words'].values)\n",
    "\n",
    "df1_int_data = tokenizer.texts_to_sequences(df1['tweet_without_stop_words'].values) # Change text into integer\n",
    "df1_int_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "dQNMucrtQwnm",
    "outputId": "49533453-50b0-4667-ef9c-bf823a8df387"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   9.,  286., 1381., 2897., 1941.,  611.,  161.,  109.,  129.,\n",
       "          83.,   87.,  105.,   77.,   35.,    9.]),\n",
       " array([  5.        ,  25.26666667,  45.53333333,  65.8       ,\n",
       "         86.06666667, 106.33333333, 126.6       , 146.86666667,\n",
       "        167.13333333, 187.4       , 207.66666667, 227.93333333,\n",
       "        248.2       , 268.46666667, 288.73333333, 309.        ]),\n",
       " <a list of 15 Patch objects>)"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For find max len \n",
    "plt.hist([len(i) for i in df1['tweet_without_stop_words']],15)\n",
    "# max_len = 80 (highest length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "E35cRCAHe10w",
    "outputId": "1f350d1c-9fd2-4aa5-e00b-015cd140f26c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0, 2072, 1213,   15,   63,   40,   21,  102,   78,\n",
       "         91,   84,    1], dtype=int32)"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding for same dimension:-\n",
    "# which padding method is imp:\n",
    "# https://stackoverflow.com/questions/46298793/how-does-choosing-between-pre-and-post-zero-padding-of-sequences-impact-results\n",
    "df1_pad_data = pad_sequences(df1_int_data, maxlen = 80)\n",
    "df1_pad_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "zvNleeTVjB1l",
    "outputId": "d6128a16-3ed2-4fc7-b5c5-bb1a55d01e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5  6  7  8  9  ...  71  72  73  74   75  76  77  78  79  label\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...  15  63  40  21  102  78  91  84   1      0\n",
      "\n",
      "[1 rows x 81 columns]\n",
      "train shape (6336, 80)\n",
      "test shape (1584, 80)\n"
     ]
    }
   ],
   "source": [
    "df_lstm = pd.DataFrame(df1_pad_data)\n",
    "df_lstm['label'] = df1_new['label']\n",
    "print(df_lstm.head(1))\n",
    "# x_dev, x_final, y_dev, y_final = train_test_split(df_lstm.iloc[:,:-1], df_lstm['label'], test_size = 0.08, random_state = 42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_lstm.iloc[:,:-1], df_lstm['label'], test_size = 0.2, random_state = 42)\n",
    "print('train shape', x_train.shape)\n",
    "print('test shape', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "-KR3zZHQe1rV",
    "outputId": "27ab6d4f-fd84-426e-feda-63f2246c78e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 80, 100)           228000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 308,501\n",
      "Trainable params: 308,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Network architecture\n",
    "model = Sequential()\n",
    "max_fatures = 2280\n",
    "maxlen = 80\n",
    "model.add(Embedding(input_dim = 2280, output_dim = 100, input_length = maxlen))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) # 100 lstm units which gives 100 output\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[get_f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "-mM0HsPre1m8",
    "outputId": "3c840b2c-9d4c-44ab-a3b1-39b3739bec1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6336/6336 [==============================] - 26s 4ms/step - loss: 0.3353 - get_f1: 0.5757\n",
      "Epoch 2/5\n",
      "6336/6336 [==============================] - 26s 4ms/step - loss: 0.2115 - get_f1: 0.8022\n",
      "Epoch 3/5\n",
      "6336/6336 [==============================] - 26s 4ms/step - loss: 0.1774 - get_f1: 0.8490\n",
      "Epoch 4/5\n",
      "6336/6336 [==============================] - 26s 4ms/step - loss: 0.1569 - get_f1: 0.8674\n",
      "Epoch 5/5\n",
      "5040/6336 [======================>.......] - ETA: 5s - loss: 0.1310 - get_f1: 0.8875"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 5 , batch_size = 16, verbose = 1)\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "y_pred = []\n",
    "for i in pred:\n",
    "  if i <= 0.5:\n",
    "    y_pred.append(0)\n",
    "  else:\n",
    "    y_pred.append(1)\n",
    "\n",
    "print(f1_score(y_test, np.array(y_pred).reshape(y_test.shape)))\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9mMf4YLe1hn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vXO56UxliTJS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "969rLFtJiTDa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BW49ENObQGWx"
   },
   "outputs": [],
   "source": [
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "RVYXlydzQGN2",
    "outputId": "ad71cd84-eb4b-4034-f022-19dfa5a52d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.8185654008438819\n",
      "0.3 0.8168421052631579\n",
      "0.35 0.8168421052631579\n",
      "0.4 0.8168421052631579\n",
      "0.45 0.8168421052631579\n",
      "0.5 0.8160676532769555\n",
      "0.6 0.8185654008438819\n",
      "0.7 0.8185654008438819\n"
     ]
    }
   ],
   "source": [
    "## MultiNaive Bayes:-\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "l = [0.2, 0.3 , 0.35, 0.4, 0.45, 0.5, 0.6, 0.7]\n",
    "for num in l:\n",
    "  model = MultinomialNB(alpha = num)\n",
    "  model.fit(x_train.iloc[:,:-1], y_train)\n",
    "  pred = model.predict(x_test.iloc[:,:-1])\n",
    "  y_pred = []\n",
    "  for i in pred:\n",
    "    if i <= 0.5:\n",
    "      y_pred.append(0)\n",
    "    else:\n",
    "      y_pred.append(1)\n",
    "  print(num, f1_score(y_test, np.array(y_pred).reshape(y_test.shape)))\n",
    "\n",
    "#0.8168, alpha = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ijw4UcTCQGE-",
    "outputId": "afb1faea-a4dd-4f50-d475-6a3098c86f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8168421052631579\n"
     ]
    }
   ],
   "source": [
    "# It's a Over fitted algoritham We can use it, IT's goving 0.68 f1 score at Test set\n",
    "model = MultinomialNB(alpha = 0.4)\n",
    "model.fit(x_train.iloc[:,:-1], y_train)\n",
    "pred = model.predict(x_test.iloc[:,:-1])\n",
    "y_pred = []\n",
    "for i in pred:\n",
    "  if i <= 0.5:\n",
    "    y_pred.append(0)\n",
    "  else:\n",
    "    y_pred.append(1)\n",
    "print(f1_score(y_test, np.array(y_pred).reshape(y_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fYLVvK4pQFz1",
    "outputId": "ec501085-f91a-4b8a-9d25-269f210e9d4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 2278)"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.iloc[:,:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zysdj4X2rNLF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-qXJ6uLErM8f",
    "outputId": "08a6b5e2-dd00-4a97-f516-26ca5ea3dd95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_66/Sigmoid:0' shape=(None, 1) dtype=float32>"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_output_at(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNuF68cEres_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w76uMczJrenl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx-HuSDBrekl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eOKY6VU3regr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "plu686LDdZ-M"
   },
   "outputs": [],
   "source": [
    "## Test Data:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_E8MyxuAdZ6z",
    "outputId": "0979a63f-975e-438a-d64e-bf4bf3554f11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1953, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test_sentiment-analytic.csv')\n",
    "id_ = df_test['id']\n",
    "df_test.drop('id', axis = 1, inplace = True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4w_ZC4RdZxj"
   },
   "outputs": [],
   "source": [
    "def punch(x):\n",
    "    x = re.sub(r\"[-()\\\"#/@;:@*<>{}`ð'+=~£|¦â.!%+-^$?,0-9]\",\"\", x)\n",
    "    x = re.sub('$&@*#', 'vulgar', x) # Mention in data \n",
    "    x = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", x) #expand 'k' to '000' eg. 50k to 50000\n",
    "    x = re.sub(\"\\\\n\", ' ', x)\n",
    "    x = re.sub(\"\\W\", ' ', x)\n",
    "    x = re.sub(\"\\'s\", \" \", x) \n",
    "    x = re.sub(\"whats\", \"what is\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(\"\\'ve\", \"have\", x)\n",
    "    x = re.sub(\"can't\", \"can not\", x)\n",
    "    x = re.sub(\"n't\", \"not\", x)\n",
    "    x = re.sub(\"i'm\", \"i am\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"\\'re\", \"are\", x)\n",
    "    x = re.sub(\"\\'d\", \"would\", x)\n",
    "    x = re.sub(r\"http\\S+\", \"\", str(x))\n",
    "    x = re.sub(\"e\\.g\\.\", \"eg\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"e-mail\", \"email\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(r\"e - mail\", \"email\", x)\n",
    "    x = re.sub(r\"US\", \"America\", x)\n",
    "    x = re.sub(r\"USA\", \"America\", x)\n",
    "    x = re.sub(r\"us\", \"America\", x)\n",
    "    x = re.sub(r\"usa\", \"America\", x)\n",
    "    x = re.sub(r\"Chinese\", \"China\", x)\n",
    "    x = re.sub(r\"india\", \"India\", x)\n",
    "    x = re.sub(\"im\", \"i am\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"its\", \"it is\", x, flags= re.IGNORECASE)\n",
    "    x = re.sub(\"shes\", \"she is\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"u\", \"you\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"ur\", \"you are\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"wouldnt\", \"would not\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(\"youve\",  \"you have\", x, flags = re.IGNORECASE)\n",
    "    x = re.sub(r\"\\s{2,}\", \" \", x) # Remove extra space between words\n",
    "    x = x.strip() # Remove extra space from begning and ending\n",
    "    \n",
    "    return(x)\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    return(' '.join([word for word in nltk.word_tokenize(x) if word not in stopwords.words('english')]))\n",
    "\n",
    "def get_char_length_ratio(x): #To find how much word has meaningful content\n",
    "    return len(x['tweet'])/max(1,len(x['tweet_without_stop_words']))\n",
    "\n",
    "def get_Levenshtein(string1,string2): # Calculate Levenshtein distance to measure similiarity between string(edit base)\n",
    "    import editdistance\n",
    "    return editdistance.eval(string1,string2)\n",
    "\n",
    "############################################################\n",
    "df_test['tweet'] = df_test['tweet'].map(lambda x : punch(x))\n",
    "df_test['tweet_without_stop_words'] = df_test['tweet'].map(lambda x : remove_stopwords(x))\n",
    "df_test['char_length_ratio'] = df_test.apply(lambda x : get_char_length_ratio(x), axis = 1)\n",
    "df_test['leve_distance'] = df_test.apply(lambda x : get_Levenshtein(x['tweet'], x['tweet_without_stop_words']), axis = 1)\n",
    "\n",
    "## Lexicon model based rating:-\n",
    "from nltk.corpus import opinion_lexicon\n",
    "pos_list = set(opinion_lexicon.positive())\n",
    "neg_list = set(opinion_lexicon.negative())\n",
    "\n",
    "sentiment = []\n",
    "for x in df_test['tweet_without_stop_words']:\n",
    "    marks = 0\n",
    "    for j in nltk.word_tokenize(x):\n",
    "        if j in pos_list:\n",
    "            marks += 1\n",
    "        elif j in neg_list:\n",
    "            marks -= 1\n",
    "    sentiment.append(marks)\n",
    "df_test['lexicon_rating'] = sentiment\n",
    "##############################################################\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', max_df = 0.99, strip_accents = 'unicode', ngram_range=(1, 1), min_df = 5) \n",
    "temp = tfidf.fit_transform(df_test['tweet'])\n",
    "\n",
    "df1_new = pd.DataFrame(temp.toarray())\n",
    "df1_new['char_length_ratio'] = df_test['char_length_ratio']\n",
    "df1_new['leve_distance'] = df_test['leve_distance']\n",
    "df1_new['lexicon_rating'] = df_test['lexicon_rating']\n",
    "# df1_new['label'] = df_test['label']\n",
    "\n",
    "# model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# f1_score(y_test, np.array(y_pred).reshape(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qqyWZ6z0dZnu",
    "outputId": "7b79421a-da6d-42d8-8769-73a503d5758e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1953, 734)\n"
     ]
    }
   ],
   "source": [
    "print(df1_new.shape)\n",
    "# df1_new = df1_new.iloc[:,:-1] # For MultiNaive Bayes\n",
    "# df1_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "GMOGcnCvyWOu",
    "outputId": "2f8ba54e-b81f-4244-a2c2-0c1c8e517deb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>694</th>\n",
       "      <th>695</th>\n",
       "      <th>696</th>\n",
       "      <th>697</th>\n",
       "      <th>698</th>\n",
       "      <th>699</th>\n",
       "      <th>700</th>\n",
       "      <th>701</th>\n",
       "      <th>702</th>\n",
       "      <th>703</th>\n",
       "      <th>704</th>\n",
       "      <th>705</th>\n",
       "      <th>706</th>\n",
       "      <th>707</th>\n",
       "      <th>708</th>\n",
       "      <th>709</th>\n",
       "      <th>710</th>\n",
       "      <th>711</th>\n",
       "      <th>712</th>\n",
       "      <th>713</th>\n",
       "      <th>714</th>\n",
       "      <th>715</th>\n",
       "      <th>716</th>\n",
       "      <th>717</th>\n",
       "      <th>718</th>\n",
       "      <th>719</th>\n",
       "      <th>720</th>\n",
       "      <th>721</th>\n",
       "      <th>722</th>\n",
       "      <th>723</th>\n",
       "      <th>724</th>\n",
       "      <th>725</th>\n",
       "      <th>726</th>\n",
       "      <th>727</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>char_length_ratio</th>\n",
       "      <th>leve_distance</th>\n",
       "      <th>lexicon_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.419114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.104478</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 734 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3  ...  730  char_length_ratio  leve_distance  lexicon_rating\n",
       "0  0.0  0.0  0.0  0.0  ...  0.0           1.104478              7              -1\n",
       "\n",
       "[1 rows x 734 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_new.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "54JjvW94uttz",
    "outputId": "e285c1af-8975-4979-ff1b-57d8dce49b5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1545"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2279-734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5JgLRiR8tq4W",
    "outputId": "d9951d48-042e-4104-a710-836d375776d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1953, 2279)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.concatenate([np.concatenate([df1_new.iloc[:,:731].values, np.zeros((1953, 1545), dtype = np.float)],axis = 1), df1_new.iloc[:, -3:]], axis = 1)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5q7k7H0KeG3a"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)\n",
    "\n",
    "y_pred = []\n",
    "for i in pred:\n",
    "    if i <= 0.5:\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8ezywzAeG0b"
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'id':id_})\n",
    "result['label'] = y_pred\n",
    "result.to_csv('result4_test_analytic.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PsyaI1peGvn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5OqrvH4veGsc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL model .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
